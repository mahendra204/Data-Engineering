{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22902783-d363-4202-b5e4-10d6feccf834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21561715-4895-4825-9896-d3167dd8d1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, 'mahendra1', 34, 5600, 'Data Science'),\n",
    "    (2, 'mahi', 35, 62000, 'Data Engineer'),\n",
    "    (3, 'am1', 29, 5400, 'Data Science'),\n",
    "    (4, 'aga', 53, 80000, 'Data Engineer'),\n",
    "    (5, 'agahow1', 25, 4400, 'Data')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48306330-9e04-4c40-998a-78107f9a9b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('first').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4524385-c6c7-44d4-a530-c5761aaddbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b804697-affc-414a-b96f-2bf2effd60f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-----+-------------+\n| _1|      _2| _3|   _4|           _5|\n+---+--------+---+-----+-------------+\n|  1|mahendra| 34|56000| Data Science|\n|  2|    mahi| 35|62000|Data Engineer|\n|  3|      am| 29|54000| Data Science|\n|  4|     aga| 53|80000|Data Engineer|\n|  5|  agahow| 25|44000|         Data|\n+---+--------+---+-----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca41fe57-a521-4b1f-a87f-1c2e99a8e88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: [Row(_1=1, _2='mahendra', _3=34, _4=56000, _5='Data Science'),\n Row(_1=2, _2='mahi', _3=35, _4=62000, _5='Data Engineer'),\n Row(_1=3, _2='am', _3=29, _4=54000, _5='Data Science'),\n Row(_1=4, _2='aga', _3=53, _4=80000, _5='Data Engineer'),\n Row(_1=5, _2='agahow', _3=25, _4=44000, _5='Data')]"
     ]
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33966038-571a-43e1-a04f-202b053cacfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = ['empid', 'name', 'age', 'salary', 'department']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d77d4adf-b0f5-4bef-ad51-e46e2fbdb014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5 = spark.createDataFrame(data,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75831183-616a-4723-a65d-cdf55f28eed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9266bae3-f528-4de3-9948-86e7aa9f7c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+-------------+\n|empid|name|age|salary|   department|\n+-----+----+---+------+-------------+\n|    2|mahi| 35| 62000|Data Engineer|\n|    4| aga| 53| 80000|Data Engineer|\n+-----+----+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.filter((col('name') == 'mahi') | (col('salary')>60000) & (col('department') == 'Data Engineer')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a341fc79-3b31-4d77-9c6a-6b363d807fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb36ed02-cb30-4656-adb8-44519d2d20a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n|    3|      am| 29| 54000| Data Science|\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.orderBy('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ede039e-b4aa-4388-b581-e9ca23015593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    5|  agahow| 25| 44000|         Data|\n|    4|     aga| 53| 80000|Data Engineer|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    1|mahendra| 34| 56000| Data Science|\n|    3|      am| 29| 54000| Data Science|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.orderBy(col('department').asc(), col('age').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a48acfd-a7dd-4a2c-9ca6-f6e9258fa0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+\n|empid|    name|age|salary|\n+-----+--------+---+------+\n|    1|mahendra| 34| 56000|\n|    2|    mahi| 35| 62000|\n|    3|      am| 29| 54000|\n|    4|     aga| 53| 80000|\n|    5|  agahow| 25| 44000|\n+-----+--------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.drop(col('department')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012af437-dbbb-4717-af46-2c098fa65b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: DataFrame[empid: bigint, name: string, age: bigint, salary: bigint, department: string]"
     ]
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cc6eb8-cbbe-453f-a5ce-966d633136d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6652b943-5a5d-4880-931d-bc39366b7b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.join(df, df2.empid == df._1).select(df._1.alias('employeeid'),\n",
    "                                        df._2.alias('employeename'),\n",
    "                                        df._3.alias('age'),\n",
    "                                        df._4.alias('salary'),\n",
    "                                        df._5.alias('department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a664af-0e88-489d-853c-7c632c5ce45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+------+-------------+\n|employeeid|employeename|age|salary|   department|\n+----------+------------+---+------+-------------+\n|         1|    mahendra| 34| 56000| Data Science|\n|         2|        mahi| 35| 62000|Data Engineer|\n|         3|          am| 29| 54000| Data Science|\n|         4|         aga| 53| 80000|Data Engineer|\n|         5|      agahow| 25| 44000|         Data|\n+----------+------------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750e39ab-f540-470b-84c3-a4fd5012b665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4 = df2.unionAll(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d97309d-0ac7-450a-8672-8ad1a3d5a392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c042632b-3494-4703-b60b-fab38a5a5290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5960ca70-d245-46a1-8697-342cbaa6752c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8862ecef-20f5-49c7-9652-3c7f8b97598f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n|    3|      am| 29| 54000| Data Science|\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.dropDuplicates(['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec0e8f6-8a15-427c-b5f1-11d24462068b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b40180e-c9d0-4e67-a6de-78a778149e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = df2.unionAll(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6bacfe-72e3-4ef7-bc83-4b55dddca36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf8ecf4-97a9-4f11-be60-3c0e08370036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d47a88e9-f4dc-45ac-9062-d59f87ec314a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5220ca1c-1e0b-4d28-9d1b-cabc5f2dab3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    5|   agahow| 25| 44000|         Data|\n|    5|  agahow1| 25|  4400|         Data|\n|    3|       am| 29| 54000| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    1| mahendra| 34| 56000| Data Science|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.drop_duplicates(['age', 'name', 'salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efcb5f4-ee6d-4dc7-8e39-13032836bf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2fd7913-001a-4997-a397-5130abefc83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96374de4-0734-401a-9770-a00167b292f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+------------+\n|empid|     name|age|salary|  department|\n+-----+---------+---+------+------------+\n|    1|mahendra1| 34|  5600|Data Science|\n+-----+---------+---+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.where(col('name')=='mahendra1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9760bca-643a-43f7-bfb5-ba48a3d63f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n|   department|total_salary|\n+-------------+------------+\n| Data Science|      121000|\n|Data Engineer|      284000|\n|         Data|       48400|\n+-------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy('department').agg(sum('salary').alias('total_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80789631-b534-48b5-b711-96bca7249cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n|   department|average_salary|\n+-------------+--------------+\n| Data Science|       55000.0|\n|Data Engineer|       71000.0|\n|         Data|       24200.0|\n+-------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.where((col('department') == 'Data')| (col('salary') > 50000))\\\n",
    "    .groupBy('department').agg(avg('salary').alias('average_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40a24a3-df82-4511-8a39-412de2b20dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[90]: 10"
     ]
    }
   ],
   "source": [
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62c9ea7-9823-4bf1-8791-74ed93b528db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[91]: DataFrame[empid: bigint, name: string, age: bigint, salary: bigint, department: string]"
     ]
    }
   ],
   "source": [
    "df6.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0402b839-991d-41f4-8679-e9528a5bbecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4a42d5-ff4d-48bc-83a1-fcdac9413250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n|   department|count|\n+-------------+-----+\n|         Data|    2|\n| Data Science|    4|\n|Data Engineer|    4|\n+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df6.groupBy('department').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49de7d00-9e4a-4903-b705-400d8cbb0528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df7 = df6.groupBy('department').agg(collect_list('name').alias('employees_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb73a3f-cabe-4992-8439-485e25d7911b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n|   department|      employees_list|\n+-------------+--------------------+\n| Data Science|[mahendra, am, ma...|\n|Data Engineer|[mahi, aga, mahi,...|\n|         Data|   [agahow, agahow1]|\n+-------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e1bde0b-3d1b-4af4-8d36-845d7b895b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------+\n|department   |employees_list                |\n+-------------+------------------------------+\n|Data Science |[mahendra, am, mahendra1, am1]|\n|Data Engineer|[mahi, aga, mahi, aga]        |\n|Data         |[agahow, agahow1]             |\n+-------------+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dde7fa7-701d-4737-a968-293fd240cc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n|department   |name     |\n+-------------+---------+\n|Data Science |mahendra |\n|Data Science |am       |\n|Data Science |mahendra1|\n|Data Science |am1      |\n|Data Engineer|mahi     |\n|Data Engineer|aga      |\n|Data Engineer|mahi     |\n|Data Engineer|aga      |\n|Data         |agahow   |\n|Data         |agahow1  |\n+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df7.withColumn('name', explode(col('employees_list'))).drop(col('employees_list')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f853b92-8fee-465f-a8cd-0d01220be376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764a9b36-c9d7-499c-81e4-d41db489d850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df6.write.mode('overwrite').format('csv').save('dbfs:/FileStore/tables/employees_info', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fb7924-d0f2-401d-8c07-7477499d0f4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "d = spark.read.csv('dbfs:/FileStore/tables/employees_info', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f534ea64-7771-4569-877b-72d05c996b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    5|   agahow| 25| 44000|         Data|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74464011-07d8-4433-8acb-bc60af93c510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s= spark.read.format('csv').option('header', True)\\\n",
    "    .load('dbfs:/FileStore/tables/employees_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f422781-daef-400a-a878-4f2ceb69be09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    5|   agahow| 25| 44000|         Data|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f7c97e-8cfb-409a-bae7-37b03332460d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s.write.format('parquet').option('header',True).save('dbfs:/FileStore/tables/employees_info1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62912e86-d92d-438c-9d66-c0e49dc6c4fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    2|     mahi| 35| 62000|Data Engineer|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    4|      aga| 53| 80000|Data Engineer|\n|    1| mahendra| 34| 56000| Data Science|\n|    1|mahendra1| 34|  5600| Data Science|\n|    3|       am| 29| 54000| Data Science|\n|    3|      am1| 29|  5400| Data Science|\n|    5|   agahow| 25| 44000|         Data|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('dbfs:/FileStore/tables/employees_info1', header = True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963a4def-535e-4b7f-b11e-d1301910aa94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+--------------+--------------+-------------------+\n|   department|minimum_salary|maximum_salary|average_salary|number_of_employees|\n+-------------+--------------+--------------+--------------+-------------------+\n| Data Science|          5400|         56000|       30250.0|                  4|\n|Data Engineer|         62000|         80000|       71000.0|                  4|\n|         Data|          4400|         44000|       24200.0|                  2|\n+-------------+--------------+--------------+--------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.groupby(col('department')).agg(min(col('salary')).alias('minimum_salary'),\n",
    "                                   max(col('salary')).alias('maximum_salary'), \n",
    "                                   avg('salary').alias('average_salary'),\n",
    "                                   count('empid').alias('number_of_employees')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94099bfd-1e3e-4cdb-be82-b6b624c434e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b56c3ef2-510f-416c-a0ab-1e8d08aa5c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df8 = df6.withColumn('age_group', when(col('age')<=30, 'Young')\\\n",
    "    .when((col('age')>30) & (col('age')<50), 'middle').otherwise('Old'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3aee46f-d049-4ac9-8106-1fcfc85cd184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+---------+\n|empid|     name|age|salary|   department|age_group|\n+-----+---------+---+------+-------------+---------+\n|    1| mahendra| 34| 56000| Data Science|   middle|\n|    2|     mahi| 35| 62000|Data Engineer|   middle|\n|    3|       am| 29| 54000| Data Science|    Young|\n|    4|      aga| 53| 80000|Data Engineer|      Old|\n|    5|   agahow| 25| 44000|         Data|    Young|\n|    1|mahendra1| 34|  5600| Data Science|   middle|\n|    2|     mahi| 35| 62000|Data Engineer|   middle|\n|    3|      am1| 29|  5400| Data Science|    Young|\n|    4|      aga| 53| 80000|Data Engineer|      Old|\n|    5|  agahow1| 25|  4400|         Data|    Young|\n+-----+---------+---+------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df8.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d76bf8-4d45-4ab1-b55a-3ff90f75b9d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75a58ec-f7c3-4425-ad22-010cc652c077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+------+-------------+----------------+\n|employeeid|employeename|age|salary|   department|cumilative_total|\n+----------+------------+---+------+-------------+----------------+\n|         1|    mahendra| 34| 56000| Data Science|           56000|\n|         2|        mahi| 35| 62000|Data Engineer|          118000|\n|         3|          am| 29| 54000| Data Science|          172000|\n|         4|         aga| 53| 80000|Data Engineer|          252000|\n|         5|      agahow| 25| 44000|         Data|          296000|\n+----------+------------+---+------+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "wind_sp = Window.orderBy('employeeid')\n",
    "df3.withColumn('cumilative_total', sum('salary').over(wind_sp)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50621197-d478-483b-9764-4f231a4308be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|     296000|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df3.agg(sum(col('salary'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d108d56b-d2a6-4377-a0f3-59c63ab4798d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+---+------+-------------+----------+\n|employeeid|employeename|age|salary|   department|dept_total|\n+----------+------------+---+------+-------------+----------+\n|         5|      agahow| 25| 44000|         Data|     44000|\n|         2|        mahi| 35| 62000|Data Engineer|     62000|\n|         4|         aga| 53| 80000|Data Engineer|    142000|\n|         1|    mahendra| 34| 56000| Data Science|     56000|\n|         3|          am| 29| 54000| Data Science|    110000|\n+----------+------------+---+------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "wind_sp1 = Window.partitionBy('department').orderBy('employeeid')\n",
    "df3.withColumn('dept_total',sum(col('salary')).over(wind_sp1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6933fd4e-0d45-49cc-98bd-688eb4ddc185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+\n|empid|     name|age|salary|   department|\n+-----+---------+---+------+-------------+\n|    1| mahendra| 34| 56000| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|       am| 29| 54000| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|   agahow| 25| 44000|         Data|\n|    1|mahendra1| 34|  5600| Data Science|\n|    2|     mahi| 35| 62000|Data Engineer|\n|    3|      am1| 29|  5400| Data Science|\n|    4|      aga| 53| 80000|Data Engineer|\n|    5|  agahow1| 25|  4400|         Data|\n+-----+---------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba17ddc5-5c5e-462e-b464-40a2c9bb8cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---+------+-------------+---------+\n|empid|     name|age|salary|   department|age_group|\n+-----+---------+---+------+-------------+---------+\n|    1| mahendra| 34| 56000| Data Science|   middle|\n|    2|     mahi| 35| 62000|Data Engineer|   middle|\n|    3|       am| 29| 54000| Data Science|    Young|\n|    4|      aga| 53| 80000|Data Engineer|      Old|\n|    5|   agahow| 25| 44000|         Data|    Young|\n|    1|mahendra1| 34|  5600| Data Science|   middle|\n|    2|     mahi| 35| 62000|Data Engineer|   middle|\n|    3|      am1| 29|  5400| Data Science|    Young|\n|    4|      aga| 53| 80000|Data Engineer|      Old|\n|    5|  agahow1| 25|  4400|         Data|    Young|\n+-----+---------+---+------+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39dfc1e0-f62a-4720-b6a3-580d4ba5cc14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+------+-------------+---+\n|empid|name|age|salary|   department|rnk|\n+-----+----+---+------+-------------+---+\n|    4| aga| 53| 80000|Data Engineer|  1|\n|    4| aga| 53| 80000|Data Engineer|  1|\n|    2|mahi| 35| 62000|Data Engineer|  2|\n|    2|mahi| 35| 62000|Data Engineer|  2|\n+-----+----+---+------+-------------+---+\n\n"
     ]
    }
   ],
   "source": [
    "sec_h = Window.orderBy(col('salary').desc())\n",
    "df6.withColumn('rnk', dense_rank().over(sec_h)).where(col('rnk')<=2).show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3da1b5e-7f5b-48ae-b0be-3255a7ea9243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd499520-2734-4d15-a9ed-5211a77d833a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfy =df2.withColumnRenamed('empid', 'employeeid').withColumnRenamed('name', \n",
    "                                                               'employeename')\\\n",
    "                                                                   .withColumnRenamed('age', 'employeeage')\\\n",
    "                                                                       .withColumnRenamed('salary', 'employee_salary')\\\n",
    "                                                                           .withColumnRenamed('department', 'department_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072b6e2b-99ee-47c4-96ad-b193412300c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ff08dd-aa45-4eda-a469-5e3c669d3cd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfx = df2.withColumn('mgrid', when(col('department')=='Data Engineer', 2)\\\n",
    "    .when(col('department')=='Data Science', 1).otherwise(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d35373-0398-4ef9-8132-7d4b29da711b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+-----+\n|empid|    name|age|salary|   department|mgrid|\n+-----+--------+---+------+-------------+-----+\n|    1|mahendra| 34| 56000| Data Science|    1|\n|    2|    mahi| 35| 62000|Data Engineer|    2|\n|    3|      am| 29| 54000| Data Science|    1|\n|    4|     aga| 53| 80000|Data Engineer|    2|\n|    5|  agahow| 25| 44000|         Data|    3|\n+-----+--------+---+------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dfx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d548ab69-efc8-4750-a1a5-153b83de45ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+-----+\n|empid|    name|age|salary|   department|mgrid|\n+-----+--------+---+------+-------------+-----+\n|    1|mahendra| 34| 56000| Data Science|    1|\n|    2|    mahi| 35| 62000|Data Engineer|    2|\n|    3|      am| 29| 54000| Data Science|    1|\n|    4|     aga| 53| 80000|Data Engineer|    2|\n|    5|  agahow| 25| 44000|         Data|    3|\n+-----+--------+---+------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dfx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a17f6b-d416-4ec9-b9fb-52451179d2bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+\n|empid|    name|age|salary|   department|\n+-----+--------+---+------+-------------+\n|    1|mahendra| 34| 56000| Data Science|\n|    2|    mahi| 35| 62000|Data Engineer|\n|    3|      am| 29| 54000| Data Science|\n|    4|     aga| 53| 80000|Data Engineer|\n|    5|  agahow| 25| 44000|         Data|\n+-----+--------+---+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6321f974-6020-47c4-8474-ee6e89457d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2755170561989769>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdfx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdfx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmgrid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdf2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempid\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mselect(dfx\u001B[38;5;241m.\u001B[39mempid)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n",
       "\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n",
       "\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Column empid#6625L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2755170561989769>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdfx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdfx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmgrid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdf2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempid\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mselect(dfx\u001B[38;5;241m.\u001B[39mempid)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column empid#6625L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Column empid#6625L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfx.join(df2, dfx.mgrid == df2.empid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a7dfbb-6aad-417b-ae3b-18a800be0f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---+------+-------------+-----+\n|empid|    name|age|salary|   department|mgrid|\n+-----+--------+---+------+-------------+-----+\n|    1|mahendra| 34| 56000| Data Science|    1|\n|    2|    mahi| 35| 62000|Data Engineer|    2|\n|    3|      am| 29| 54000| Data Science|    1|\n|    4|     aga| 53| 80000|Data Engineer|    2|\n|    5|  agahow| 25| 44000|         Data|    3|\n+-----+--------+---+------+-------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "dfx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0d6c25-cb4b-46fe-9190-1018eba8d598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2755170561989771>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m a \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m b \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 3\u001B[0m a\u001B[38;5;241m.\u001B[39mjoin(b, a\u001B[38;5;241m.\u001B[39mmgrid \u001B[38;5;241m==\u001B[39m b\u001B[38;5;241m.\u001B[39mempid)\u001B[38;5;241m.\u001B[39mselect(a[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m], b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmanager_name\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2755170561989771>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m a \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m b \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m a\u001B[38;5;241m.\u001B[39mjoin(b, a\u001B[38;5;241m.\u001B[39mmgrid \u001B[38;5;241m==\u001B[39m b\u001B[38;5;241m.\u001B[39mempid)\u001B[38;5;241m.\u001B[39mselect(a[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m], b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmanager_name\u001B[39m\u001B[38;5;124m'\u001B[39m))\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = dfx.alias('a')\n",
    "b = dfx.alias('b')\n",
    "a.join(b, a.mgrid == b.empid).select(a['*'], b['name'].alias('manager_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05fb5b3f-7aa3-42d2-bbe3-b21101c0f104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2755170561989772>:4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m a \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m b \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 4\u001B[0m result \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mjoin(b, a\u001B[38;5;241m.\u001B[39mmgrid \u001B[38;5;241m==\u001B[39m b\u001B[38;5;241m.\u001B[39mempid) \\\n",
       "\u001B[1;32m      5\u001B[0m           \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;241m*\u001B[39m[a[col] \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m dfx\u001B[38;5;241m.\u001B[39mcolumns], b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmanager_name\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
       "\u001B[1;32m      7\u001B[0m result\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n",
       "\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   2980\u001B[0m \n",
       "\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n",
       "\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2755170561989772>:4\u001B[0m\n\u001B[1;32m      1\u001B[0m a \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m b \u001B[38;5;241m=\u001B[39m dfx\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m result \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mjoin(b, a\u001B[38;5;241m.\u001B[39mmgrid \u001B[38;5;241m==\u001B[39m b\u001B[38;5;241m.\u001B[39mempid) \\\n\u001B[1;32m      5\u001B[0m           \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;241m*\u001B[39m[a[col] \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m dfx\u001B[38;5;241m.\u001B[39mcolumns], b[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmanager_name\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      7\u001B[0m result\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3023\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m   2978\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mselect\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mColumnOrName\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   2979\u001B[0m     \u001B[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   2980\u001B[0m \n\u001B[1;32m   2981\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3021\u001B[0m \u001B[38;5;124;03m    +-----+---+\u001B[39;00m\n\u001B[1;32m   3022\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3023\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jcols\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcols\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3024\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Column name#152 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da9e0cb6-03cb-45a3-ab7c-f1f99d28e6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Revision_Scripting",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}